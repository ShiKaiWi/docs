# 集群一致性

## 背景

在 [集群](./clustering.md) 文章中介绍了 CeresDB 的集群方案，简单总结一下就是：

- 计算存储分离；
- 由中心化的元数据中心，管理整个集群；

然而计算存储分离的架构下，有个重要的问题在于如何解决数据一致性的问题，更直白地说，就是如何保证计算节点在共享存储的情况下，不会导致数据错乱，这个问题是计算存储分离方案的面临的一个基本问题。

CeresDB 的解决方案是通过特定的机制，达到在共享存储的情况下构成 [Share-nothing 架构](https://en.wikipedia.org/wiki/Shared-nothing_architecture) 的效果，也就是说存储层的数据经过一定规则的划分，**可以保证在任何时刻最多只有一个 CeresDB 实例可以对其进行更新**（这就是本文所说的集群一致性），这样的话，数据的一致性就会得到天然的保障。然而问题在于，如何保证划分好的数据在任何时刻最多只有一个计算节点对其进行更新呢？下面将会详细介绍一下 CeresDB 是如何通过特定的机制，来保证这一点的。

## 数据划分

在 [之前的文章](./clustering.md#shard) 介绍了 Shard 的基本作用，作为集群的基本调度单元，实际上也是数据分布的划分单元，也就是说不同的 Shard 在存储层的数据是隔离的：

- 在 WAL 中，写入的 Table 数据会按照 Shard 组织起来，按照 Shard 写入到 WAL 的不同区域中，不同的 Shard 在 WAL 中的数据是隔离开的；
- 在 Object Storage 中，数据的管理是按照 Table 来划分的，而 Shard 和 Table 之间的关系是一对多的关系，也就说，任何一个 Table 只属于一个 Shard，因此在 Object Storage 中，Shard 之间的数据也是隔离的；

## Shard Lock

在数据划分好之后，需要保证的是在任何时刻，对于 Shard 的数据更新最多只有一个计算节点。那么要如何保证这一点的呢？很自然地，通过锁可以达到互斥的效果，不过在分布式集群中，我们需要的是分布式锁。通过分布式锁，每一个 Shard 被分配给 CeresDB 实例时，CeresDB 必须先获取到相应的 Shard Lock，才能完成 Shard 的打开操作，当然，Shard 关闭后，也需要主动释放 Shard Lock。

CeresDB 集群的元数据服务 CeresMeta 是基于 ETCD 构建的，而基于 ETCD 实现分布式的 Shard Lock 是非常方便的：

- 以 Shard ID 作为 ETCD 的 Key，获取到 Shard Lock 等价于创建出这个 Key；
- 对应的 Value，可以把 CeresDB 的地址编码进去（用于 CeresMeta 调度）；
- Shard Lock 获取到了之后，需要通过 ETCD 提供的接口对其进行续租，保证 Shard Lock 不会被释放；

CeresMeta 暴露了 ETCD 的服务提供给 CeresDB 集群来构建 Shard Lock，下图展示了 Shard Lock 的工作流程，图中的两个 CeresDB 实例都尝试打开 Shard 1，但是由于 Shard Lock 的存在，使得只有一个 CeresDB 实例可以完成 Shard 1 的打开：

```
             ┌────────────────────┐
             │                    │
             │                    │
             ├───────┐            │
   ┌─────┬──▶│ ETCD  │            │
   │     │   └───────┴────CeresMeta
   │     │       ▲
   │     │       └──────┬─────┐
   │     │          Rejected  │
   │     │              │     │
┌─────┬─────┐        ┌─────┬─────┐
│Shard│Shard│        │Shard│Shard│
│  0  │  1  │        │  1  │  2  │
├─────┴─────┤        ├─────┴─────┤
└─────CeresDB        └─────CeresDB
```

## 其他方案

Shard Lock 的方案本质上是 CeresDB 通过 ETCD 来保证集群的一致性（也就是说**对于任何一个 Shard 在任何时刻最多只有一个 CeresDB 实例可以对其进行更新操作**），注意这个一致性成为了 CeresDB 实例提供的能力，CeresMeta 无需保证这一点。

除此 Shard Lock 的方案，我们还考虑过这样的两种方案：

### CeresMeta 状态同步

CeresMeta 存储集群的拓扑状态，并且保证其一致性，并负责将该一致性的状态同步到 CeresDB，CeresDB 本身无权决定 Shard 是否可以打开，只有在得到 CeresMeta 的通知后，才能打开指定的 Shard。而 CeresDB 需要不停地向 CeresMeta 发送心跳，一方面汇报自身的负载信息，一方面让 CeresMeta 知道该节点仍然在线。

该方案也是 CeresDB 一开始采用的方案，该方案的思路简洁，但是在实现过程中却是很难做好的，其难点在于，CeresMeta 在执行调度的时候，相当于是基于最新的一致状态，决策出一个新的变更，并且应用到 CeresDB 集群，但是这个变更到达某个具体的 CeresDB 实例时，即将产生效果的时候，无法保证此刻的集群状态仍然是和做出该变更时基于的那个集群状态是一致的。

让我们用更精确的语言描述一下：

```
t0: 集群状态是 S0，CeresMeta 据此计算出变更 U；
t1: CeresMeta 将 U 发送到某个 CeresDB 实例，让其执行变更；
t2: 集群状态变成 S1；
t3: CeresDB 接收到 U，准备进行变更；
```

上述的例子的问题在于，t3 时刻，CeresDB 执行变更 U 是否正确呢？这个正确性，需要 CeresMeta 相当复杂的逻辑来保证即使在集群状态为 S1 的情况下，执行 U 变更也不会出现问题。

对比 Shard Lock 的方案，可以发现，该方案尝试获得更强的一致性，即尝试保证集群的拓扑状态在任何时刻得需要和 CeresMeta 中的集群状态保持一致，显然，这样的一致性肯定是能够保证**任何一个 Shard 在任何时刻最多只有一个 CeresDB 实例**的，也正因为如此实现起来才会更加复杂，而基于 Shard Lock 的方案放弃了这样一个**已知**的一致性状态，只需要保证集群状态是一致的即可，而不需要知道这个状态究竟是什么。

## CeresDB 的一致性协议

该方案是参考 TiDB 的元数据服务 [pd](https://github.com/tikv/pd) 的，pd 管理着所有的 TiKV 数据节点，但是 pd 不需要维护一致性的集群状态，并且应用到 TiKV 节点上面去，因为 TiKV 集群中，每一个 Raft Group，都能够达到一致性，也就是说 TiKV 无需借助 PD，本身就具备了让整个集群一致的能力（一个 Raft Group 不会出现两个 Leader）。

参考该方案，我们也可以在 CeresDB 实例之间实现一致性协议，让其本身也具备这样的能力，不过在 CeresDB 之间引入一致性协议，似乎把事情变得更加复杂了，而且目前也没有需要更多的数据需要同步，而通过外部的服务（ETCD）依然可以同样的效果，而从 CeresMeta 看，就等价与 CeresDB 本身获得了让集群一致的能力。

因此 Shard Lock 的方案，可以看作是该方案的一个变种，是一种取巧但是很实用的实现。

## 总结

CeresDB 分布式方案的最终目标自然不是保持一致性就够了，但是保持一致性是后续目标的重要基石，这部分的逻辑简洁明了，有充分的理论保证，可以让后续的功能实现更加优雅。例如，为了使得 CeresDB 集群中的各个节点达到负载均衡的效果，就是必须根据 CeresDB 实例上报的消息，对集群中的节点进行调度，而调度的单位必然是 Shard，然而任何一次 Shard 变动都可能造成数据的损坏（一个 Shard 被两个实例同时打开），而有了 Shard Lock 的保证后，CeresMeta 就可以放心地生成调度计划，根据上报的负载信息，计算出当前状态的最佳调度结果，然后发送到涉及的 CeresDB 实例让其执行，即使计算的前提可能是错误的（即集群状态已经和计算出调度结果时的状态不一样了），也不用担心集群的一致性状态遭到破坏，因而 CeresMeta 的调度逻辑，变得简洁而优雅（只需要生成、执行，不需要考虑失败处理）。
